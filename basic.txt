**Project Title:** AI-Powered MCQ Practice Coach

**Project Vision:** To create a web-based learning tool that feels less like a quiz and more like a personal, one-to-one performance coach. This is an **experimental project** with the primary goal of testing the cognitive limits of a Large Language Model (LLM) in a real-time coaching scenario.

**UI/UX Design Philosophy:**
*   **Conversational Interface:** The entire user experience is modeled after a modern messaging app.
*   **Guided Interaction:** The AI presents MCQ options and prompts as clickable buttons to guide the user.
*   **User Flexibility:** A standard text input field allows the user to ask questions or type commands freely.
*   **Minimalist & Clean:** The design is clean and focused on the conversation.

**Backend Architecture: "Pure LLM-Cognition" Model**

This project will be a direct implementation of the "LLM-as-the-Brain" strategy. We will delegate almost all cognitive tasks—including conversational logic, performance analysis, and decision-making—directly to the Gemini LLM.

*   **Core Principle:** The LLM is the central cognitive engine. Our Python backend's primary role is to act as a data marshaller: it gathers all relevant context, sends it to the LLM, and executes the LLM's returned command.

*   **Cognitive Tasks of the LLM:**
    1.  **Intent Classification:** Determine the user's goal from their message (e.g., answering a question, asking for a hint).
    2.  **Performance Analysis:** Analyze the entire chat history, including message timestamps, to dynamically assess the user's accuracy, response speed, and potential areas of weakness or strength.
    3.  **Action Selection:** Choose the next action for the application to take from a predefined list (e.g., `EVALUATE_ANSWER`, `CHOOSE_NEXT_QUESTION`).
    4.  **Response Generation:** Craft a user-facing message that is consistent with a helpful, encouraging coaching philosophy.
    5.  **Structured Output:** Deliver its decision in a reliable JSON format.

*   **Gemini API Method:** We will use the stateless **`generate_content()`** method. This gives us maximum control to construct a detailed "master prompt" on every turn, containing all the context the LLM needs to perform the cognitive tasks listed above.

*   **Modular Design:** The backend will be kept lean and organized into three components:
    1.  **API Gateway (`app.py`):** Manages user sessions (chat history with timestamps) and orchestrates the data flow.
    2.  **Question Bank (`question_bank.py`):** A simple data loader for the `mcq.json` file.
    3.  **Gemini Service Wrapper (`gemini_service.py`):** Responsible for constructing the master prompt and communicating with the Gemini API.

*   **Experimental Goal & Long-Term Strategy:** The initial goal is to successfully implement this ambitious architecture. Based on the results, we will identify which tasks are best suited for the LLM and which could be offloaded to local Python code or specialized smaller models to optimize for cost, latency, and reliability in future phases.

**User Flow (LLM-Driven):**
1.  The user starts the application. The backend records the start time.
2.  The backend sends a minimal context to the LLM, which decides to greet the user and present a "Let's Begin!" button.
3.  When the user clicks "Let's Begin!", the backend records the message and its timestamp in the conversation history.
4.  The backend sends the updated history to the LLM. The LLM analyzes the history, sees the user wants to start, and decides to select the first question. It returns the question text and options.
5.  The frontend renders the question. The backend records the AI's message and its timestamp.
6.  The user answers. The backend records the user's answer and its timestamp.
7.  The backend sends the full history, including all messages with their timestamps, to the LLM.
8.  The LLM performs all cognitive tasks: it evaluates the answer's correctness, analyzes the response time by comparing the timestamps of the question and the answer, updates its internal "model" of the user's performance, formulates a coaching-style response, and decides the next best action.
9.  The backend receives the JSON decision, records the AI's new response and timestamp, and sends the response to the frontend. The cycle continues.